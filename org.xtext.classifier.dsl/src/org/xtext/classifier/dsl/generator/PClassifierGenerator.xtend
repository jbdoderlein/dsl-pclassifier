/*
 * generated by Xtext 2.26.0
 */
package org.xtext.classifier.dsl.generator

import org.eclipse.emf.ecore.resource.Resource
import org.eclipse.xtext.generator.AbstractGenerator
import org.eclipse.xtext.generator.IFileSystemAccess2
import org.eclipse.xtext.generator.IGeneratorContext

import org.xtext.classifier.dsl.pClassifier.Classifier
import org.xtext.classifier.dsl.pClassifier.Train
import org.xtext.classifier.dsl.pClassifier.Execute
import org.xtext.classifier.dsl.pClassifier.Load
import org.xtext.classifier.dsl.pClassifier.Save
import org.xtext.classifier.dsl.pClassifier.FeatureList
import org.xtext.classifier.dsl.pClassifier.MLModel
import org.xtext.classifier.dsl.pClassifier.Evaluation
import org.xtext.classifier.dsl.pClassifier.EvaluationList

class PClassifierGenerator extends AbstractGenerator {

	override void doGenerate(Resource resource, IFileSystemAccess2 fsa, IGeneratorContext context) {
		var result = '''
		import numpy as np
		import pandas as pd
		import pickle
		from sklearn.model_selection import train_test_split
		from sklearn.tree import DecisionTreeClassifier
		from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
		
		models = {}
		
		'''
		for(e : resource.allContents.toIterable()) {
			switch (e) {
				case (e instanceof Classifier): {
					result += generateClassifier(e as Classifier)
					result += "\n"
				}
				case (e instanceof Train): {
					result += generateTrain(e as Train)
					result += "\n"
				}
				case (e instanceof Execute): {
					result += generateExecute(e as Execute)
					result += "\n"
				}
				case (e instanceof Load): {
					result += generateLoad(e as Load)
					result += "\n"
				}
				case (e instanceof Save): {
					result += generateSave(e as Save)
					result += "\n"
				}
			}
		}
		fsa.generateFile('result.py',  
			result)
		
	}
	
	private def generateClassifier(Classifier classifier) '''
		class 첽lassifier.name:
			features = [쳂andleFeatures(classifier.features)]
			target = "첽lassifier.target"
			
			def __init__(self):
				쳂andleMLModel(classifier.model)
			
			def train(self, data, split, evaluations):
				train, test = train_test_split(data, test_size=split)
				self.model.fit(train[self.features], train[self.target])
				self.evaluate(test, evaluations)
				
			def evaluate(self, test, evaluations):
				results_eval = {}
				for evaluation in evaluations:
					if evaluation == 'accuracy':
						results_eval[evaluation] = accuracy_score(test[self.target], self.model.predict(test[self.features]))
					elif evaluation == 'confusion_matrix':
						results_eval[evaluation] = confusion_matrix(test[self.target], self.model.predict(test[self.features]))
					elif evaluation == 'f1_score':
						results_eval[evaluation] = f1_score(test[self.target], self.model.predict(test[self.features]))
					elif evaluation == 'recall':
						results_eval[evaluation] = recall_score(test[self.target], self.model.predict(test[self.features]))
				print(
					'첽lassifier.name training results :\n'+ 
					pd.DataFrame({'Score':list(results_eval.keys()), '':list(results_eval.values())}).to_markdown(index=False)
				)
			
			def execute(self, data, output):
				with open(output, 'w') as f:
					f.write(self.model.predict(data[self.features]).to_string(index=False))
		
		models["첽lassifier.name"] = 첽lassifier.name()
	'''
	
	private def generateTrain(Train train) '''
		df = pd.read_csv("쳓rain.dataset")
		classifier = models["쳓rain.name"].train(data=df, split=쳓rain.split, evaluations=[쳂andleEvaluationList(train.evaluations)])
	'''
	
	private def generateExecute(Execute exec) '''
		df = pd.read_csv("첿xec.input")
		models["첿xec.name"].execute(df, "첿xec.output")
	'''
	
	private def generateLoad(Load load) '''
		with open("쳊oad.file", "r") as f:
			models["쳊oad.name"] = pickle.load(f)
	'''
	
	private def generateSave(Save save) '''
		with open("쳒ave.file", "wb") as f:
			pickle.dump(models["쳒ave.name"], f)
	'''
	
	def handleFeatures(FeatureList features){
		return "\"" + features.vals.join("\",\"") + "\""
	}
	
	def handleEvaluationList(EvaluationList eval_list){
		return "\"" + eval_list.vals.join("\",\"") + "\""
	}
	
	def handleMLModel(MLModel mlmodel){
		var result = "self.model = "
		switch (mlmodel.literal){
			case "DecisionTree":{
				result += "DecisionTreeClassifier(random_state=0, max_depth=5)"
			}
			case "SVM":{
				result += "DecisionTree(random_state=0, max_depth=5)"
			}
		}
		return result
	}
	
	def handleEvaluation(Evaluation evaluation){
		return evaluation.literal
	}
}
